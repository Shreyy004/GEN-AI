{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLjfYbAfSCbW",
        "outputId": "e6bc937e-0d2c-42ff-aa6b-2b92d8667816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install cohere --quiet\n",
        "!pip install langchain_community --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMwoNQTOT_QA"
      },
      "source": [
        "#OpenAI Model - Paid Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTzpLBqhVObK"
      },
      "source": [
        "Get your OpenAI API key here\n",
        "https://platform.openai.com/usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLcmCBxxUDx_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"Your own OPENAI_API_KEY\"\n",
        "\n",
        "#Better way\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIjGxnjpUD53"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=0.9, max_tokens=256)\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)\n",
        "\n",
        "# - temperature: Set to 0.9, which controls the randomness of the output.\n",
        "#   A higher temperature results in more varied and unpredictable outputs,\n",
        "#   while a lower temperature produces more deterministic and conservative outputs.\n",
        "#   This is often used in generative tasks to balance between creativity and relevance.\n",
        "\n",
        "# - max_tokens: Set to 256, which specifies the maximum number of tokens (words or pieces of words)\n",
        "#   that the model can generate in a single response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SrvJiBPUD-v"
      },
      "outputs": [],
      "source": [
        "llm=OpenAI(temperature=0)\n",
        "response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXYuvS5jQ6Rv"
      },
      "source": [
        "#Cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhHtQ_iSQ9EW"
      },
      "source": [
        "Get your Cohere Trail API key here\n",
        "https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEFXkf--Tqn2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['COHERE_API_KEY'] = userdata.get(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cZpytJ7QsEw",
        "outputId": "784119fe-2088-4fce-f2a4-74c6b859a773"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-798555d13791>:3: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
            "  llm = Cohere(temperature=0.9, max_tokens=256)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " AI, you're such a mystery,\n",
            "A fascinating universe awaited,\n",
            "You're full of wisdom, resilience, and power,\n",
            "A partner aiding us, forever. \n",
            "\n",
            "Do you like it? I can write a longer poem if you'd like!\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import Cohere\n",
        "\n",
        "llm = Cohere(temperature=0.9, max_tokens=256)\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSczTG0-V_tY",
        "outputId": "0b94f8bc-8a0b-4387-ac03-34bdb5cfe91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Overfitting in Machine Learning is a common problem that occurs when a model becomes too complex and learns to recognize patterns that are specific to the training data, rather than general patterns that apply to new, unseen data.\n",
            "\n",
            "Think of trying to teach a statue to distinguish between different types of animals. You train it by showing it images of various animals like cats and dogs. If the statue is too complex, it will learn to identify objects that are very specific to the images you showed it, like the unique spots of a specific cat in your training set. However, it may fail to generalize and recognize other animals, or different cats and dogs it hasn't seen before. \n",
            "\n",
            "Overfitting occurs in machine learning models, especially those with many parameters or complex structures, and it can lead to disappointing performance on new data. Techniques like cross-validation and regularization are used to prevent overfitting during the model training process. \n"
          ]
        }
      ],
      "source": [
        "llm=Cohere(temperature=0)\n",
        "response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Aq4hbTgY45z"
      },
      "source": [
        "# Open source models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOi09tmebaTo"
      },
      "source": [
        "\n",
        "* Mistral Model (Mistral 7B, Mixtral8-7B)\n",
        "* LLama (Llam2, Llama3)\n",
        "* Bloom by Hugging Face\n",
        "* Falcon 180B\n",
        "* Opt 175B\n",
        "* Xgen-7B\n",
        "* Vicuna-13B\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZqC43tme-0v"
      },
      "source": [
        "### Top Open-Source Large Language Models for 2024\n",
        "\n",
        "1. **LLaMA 2**:\n",
        "   - Developed by Meta, LLaMA 2 is a generative text model with 7 to 70 billion parameters, fine-tuned with reinforcement learning from human feedback (RLHF).\n",
        "   - Released for research and commercial use in July 2023.\n",
        "   - Includes versions like LLaMA Chat and Code LLaMA for varied natural language tasks.\n",
        "\n",
        "2. **BLOOM**:\n",
        "   - Launched by Hugging Face in 2022, BLOOM is an autoregressive model with 176 billion parameters.\n",
        "   - Supports 46 languages and 13 programming languages.\n",
        "   - Emphasizes transparency and is available for free through Hugging Face.\n",
        "\n",
        "3. **BERT**:\n",
        "   - Introduced by Google in 2018, BERT is known for its bidirectional encoder representations from transformers.\n",
        "   - Achieved state-of-the-art performance in many NLP tasks and is widely used, including in Google Search.\n",
        "\n",
        "4. **Falcon 180B**:\n",
        "   - Released by the Technology Innovation Institute in the UAE in 2023.\n",
        "   - With 180 billion parameters, it rivals models like LLaMA 2 and GPT-3.5.\n",
        "   - Requires significant computing resources.\n",
        "\n",
        "5. **OPT-175B**:\n",
        "   - Part of Meta's suite of pre-trained transformers, released in 2022.\n",
        "   - Ranges from 125M to 175B parameters.\n",
        "   - Available for research use only due to its non-commercial license.\n",
        "\n",
        "6. **XGen-7B**:\n",
        "   - Launched by Salesforce in July 2023, designed for longer context windows.\n",
        "   - Utilizes only 7 billion parameters.\n",
        "   - Available for commercial and research purposes, with some variants under a non-commercial license.\n",
        "\n",
        "7. **GPT-NeoX and GPT-J**:\n",
        "   - Developed by EleutherAI, GPT-NeoX has 20 billion parameters and GPT-J has 6 billion parameters.\n",
        "   - Available for various NLP tasks via the NLP Cloud API.\n",
        "\n",
        "8. **Vicuna-13B**:\n",
        "   - Fine-tuned from LLaMA 13B, Vicuna-13B is a conversational model.\n",
        "   - Performs well in customer service, healthcare, education, and more.\n",
        "   - Achieves high quality, comparable to ChatGPT and Google Bard.\n",
        "\n",
        "### Choosing the Right Open-Source LLM\n",
        "Consider the following factors:\n",
        "- **Purpose**: Ensure the LLM's licensing fits your use case, especially for commercial purposes.\n",
        "- **Necessity**: Evaluate if an LLM is essential for your goals.\n",
        "- **Accuracy**: Larger models typically offer higher accuracy.\n",
        "- **Investment**: Consider the cost of resources for training and operating the LLM.\n",
        "- **Pre-trained Models**: Leverage existing pre-trained models for specific use cases to save resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQO2lLhSWQ0z"
      },
      "source": [
        "#HuggingFace models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NJk5-NNlIZQ"
      },
      "source": [
        "https://huggingface.co/mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8MLcURRcyaZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"Your own HUGGINGFACEHUB_API_TOKEN\"\n",
        "\n",
        "#Better way\n",
        "from google.colab import userdata\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynG1WvTVWS-P"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs={\"temperature\": 0.9, \"max_length\": 256},\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhnQDbE_c8dI"
      },
      "outputs": [],
      "source": [
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs={\"temperature\": 0.3, \"max_length\": 1000},\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"How to pick a stock based on Revenue, Profit and profit margin trends?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IUaxgdbnOhR"
      },
      "source": [
        "# Llama from Hugging Facehub\n",
        "https://huggingface.co/meta-llama\n",
        "\n",
        "* You need to fill the contact info and wait for the approval.\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bJ8n1APujqk"
      },
      "outputs": [],
      "source": [
        "repo_id=\"meta-llama/Meta-Llama-3.1-8B\"\n",
        "#Throws an error\n",
        "#The model meta-llama/Meta-Llama-3.1-8B is too large to be loaded automatically (16GB > 10GB).\n",
        "#Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs={\"temperature\": 0.9},\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What are some ways to boost creativity?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6lV6zlWWVsp"
      },
      "source": [
        "#Replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPL03Yfnr7Vx"
      },
      "source": [
        "- Run and fine-tune open-source models with Replicate's API.https://replicate.com/home\n",
        "- Deploy custom models at scale using one line of code.\n",
        "- Avoid managing infrastructure or learning machine learning details.\n",
        "- Use open-source models or package your own.\n",
        "- Choose to make models public or keep them private.\n",
        "- Start with any open-source model with just one line of code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZMqrpD6sKWy"
      },
      "source": [
        "Replciate API Token\n",
        "\n",
        "On top Left >>> Home>>Click on your id>> API Tokens\n",
        "https://replicate.com/account/api-tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhKCAyesWZlf",
        "outputId": "b912cc05-e07b-4f4b-8ff4-cf832f469a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting replicate\n",
            "  Downloading replicate-1.0.6-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from replicate) (24.2)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.11/dist-packages (from replicate) (2.11.4)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (4.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Downloading replicate-1.0.6-py3-none-any.whl (48 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: replicate\n",
            "Successfully installed replicate-1.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install replicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyQu5O3VsOd0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"REPLICATE_API_TOKEN\"] = userdata.get(\"R\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8cuUoGAs9di",
        "outputId": "6665049d-5650-41b7-bc27-a5070359863b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are some effective strategies for studying:\n",
            "\n",
            "1. **Set specific goals**: Before you start studying, define what you want to achieve. Break down large goals into smaller, manageable chunks, and prioritize the most important tasks.\n",
            "2. **Create a study schedule**: Plan out when and how long you'll study each day/week. Use a planner, calendar, or app to stay organized and on track.\n",
            "3. **Use active learning techniques**: Engage with the material by summarizing notes in your own words, creating concept maps, or making flashcards.\n",
            "4. **Practice active recall**: Test yourself on the material by trying to recall key concepts or facts without looking at your notes or other resources.\n",
            "5. **Spaced repetition**: Review material at increasingly longer intervals to help solidify it in your long-term memory.\n",
            "6. **Use different senses**: Incorporate different senses (sight, sound, touch) to learn and retain information. For example, watch videos, listen to podcasts, and take notes by hand.\n",
            "7. **Get enough sleep**: Sleep is essential for memory consolidation and learning. Aim for 7-9 hours of sleep each night.\n",
            "8. **Stay organized**: Keep all your study materials, including notes and schedules, organized and easily accessible.\n",
            "9. **Take breaks**: Take regular breaks to recharge and prevent burnout. Use breaks to do something enjoyable or relaxing.\n",
            "10. **Seek help when needed**: Don't hesitate to ask for help if you're struggling with a concept or subject. Reach out to teachers, classmates, or tutors for support.\n",
            "11. **Use technology strategically**: Utilize digital tools, such as flashcard apps or note-taking software, to streamline your studying and stay organized.\n",
            "12. **Review and reflect**: Regularly review what you've learned and reflect on what works best for you. Adjust your study strategies as needed.\n",
            "13. **Use mnemonic devices**: Create associations, acronyms, or rhymes to help remember key information.\n",
            "14. **Teach someone else**: Teaching someone else what you've learned is an effective way to reinforce your own understanding and retain information.\n",
            "15. **Stay motivated**: Celebrate your progress, reward yourself for milestones achieved, and remind yourself of your goals to stay motivated and engaged.\n",
            "\n",
            "Remember, everyone learns differently, so it's essential to experiment with different strategies to find what works best for you. Good luck!\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import Replicate\n",
        "\n",
        "replicate_llm = Replicate(\n",
        "    model=\"meta/meta-llama-3.1-405b-instruct\",\n",
        "    model_kwargs={\"temperature\": 0.6},\n",
        ")\n",
        "\n",
        "response = replicate_llm.invoke(\"What are some good strategies for studying?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9G5F5DWTqf"
      },
      "source": [
        "# Groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq-Y3ae0oX7B"
      },
      "source": [
        "* Developed the LPU(Language Processing Unit) chip to run LLMs faster and cheaper.\n",
        "* Offers Groq Cloud to try open-source LLMs like Llama3 or Mixtral.\n",
        "* Allows free use of Llama3 or Mixtral in apps via Groq API Key with rate limits.\n",
        "* Models on Groq https://console.groq.com/docs/models\n",
        "* Get your Groq API key https://console.groq.com/keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilkVBfuso1jK",
        "outputId": "bafc5a40-831f-4d47-c93d-6e23185384d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.59)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.25.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.13.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.4.0)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading groq-0.25.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.25.0 langchain-groq-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLICC6MpWVLA"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JCgb4ubpAy6",
        "outputId": "2f05d2bb-7c1e-4e67-96a6-917f25e3cc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Here are some thought-provoking quotes about ignorance:\\n\\n1. **\"The highest form of ignorance is when you reject something and you don\\'t even know what it is.\"** - Wayne Dyer**\\n2. **\"The only thing more exhausting than being ignorant is being aware.\"** - Unknown**\\n3. **\"Ignorance is the softest of all evils; a kind of featherbed, that, when once we are laid down in it, we are very loath to rise up again.\"** - John Dryden**\\n4. **\"The ignorance of one voter in a democracy impairs the security of all.\"** - John F. Kennedy**\\n5. **\"The highest form of wisdom is to understand that all is nothing.\"** - William Shakespeare (on the limits of knowledge)**\\n6. **\"There is no darkness but ignorance.\"** - (a play on the phrase \"there is no evil but sin\")\\n7. **\"We are all ignorant, only on different subjects.\"** - Will Rogers**\\n8. **\"Ignorance, when it is voluntary, is criminal, and it may be positively wicked.\"** - Agnes Repplier**\\n9. **\"The greatest obstacle to discovery is not ignorance - it is the illusion of knowledge.\"** - Daniel J. Boorstin**\\n10. **\"Where ignorance is our master, there is no possibility of real peace.\"** - Dalai Lama XIV**\\n\\nThese quotes highlight the importance of recognizing and addressing ignorance, as it can lead to harm, misunderstandings, and stagnation.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 320, 'prompt_tokens': 20, 'total_tokens': 340, 'completion_time': 1.397500169, 'prompt_time': 0.000286337, 'queue_time': 0.229742573, 'total_time': 1.397786506}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run--243e35f6-bbda-4ef7-8aeb-ee837da1a677-0' usage_metadata={'input_tokens': 20, 'output_tokens': 320, 'total_tokens': 340}\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm=ChatGroq(\n",
        "    model=\"llama3-70b-8192\"\n",
        ")\n",
        "result=llm.invoke(\"what are the top 10 quotes about ignorance?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDcBv9MtlU5Y"
      },
      "source": [
        "# Many more ways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr1NfzpUlXY4"
      },
      "source": [
        "https://python.langchain.com/v0.1/docs/integrations/llms/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4JPXjq1eBY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
