{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yb172S4aDFKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cc8c0b-c8c2-41e3-bbb7-73f425de5195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install cohere --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install langchain_community --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iE1mMtv1C5NM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ['OPENAI_API_KEY'] = \"Your own OPENAI_API_KEY\"\n",
        "# os.environ['COHERE_API_KEY'] = \"Your own COHERE_API_KEY\"\n",
        "\n",
        "#Better way\n",
        "from google.colab import userdata\n",
        "# os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['COHERE_API_KEY'] = userdata.get(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pY6qEt4kY4gZ"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import Cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btiPFnoDK5yp"
      },
      "source": [
        "# Memory in Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BO6wD-TLYVPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1a35fd-369f-48fa-84e1-f52c7a49f587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-972931cc225f>:1: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
            "  llm= Cohere(temperature=0, max_tokens=512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India won the T20 Cricket World Cup in 2007, 2012, and 2021. \n",
            "\n",
            "In 2007, India won the first T20 Cricket World Cup hosted in South Africa. They defeated their arch-rivals Pakistan by 5 runs in a thrilling final at Johannesburg, securing their first T20 World Cup victory. \n",
            "\n",
            "In 2012, India hosted the T20 World Cup jointly with Sri Lanka and Bangladesh. India defeated Sri Lanka in the final at the Wankhede Stadium in Mumbai by 6 wickets and secured their second T20 World Cup victory. \n",
            "\n",
            "Most recently, in 2021, Australia hosted the T20 World Cup. India secured their third T20 World Cup victory after defeating Pakistan in a nail-biting final by 5 wickets in Dubai. \n",
            "\n",
            "These three victories highlight India's dominance in T20 cricket and their ability to consistently perform at a high level in the shortest format of the game. \n",
            " Sure, unfortunately I am unable to identify the team you are referring to. I am an AI assistant trained to answer questions on general information topics. Could you please provide more information about the team you are interested in, such as the sport they play or the league they participate in, and I will be happy to assist you? Some popular teams in various sports include:\n",
            "\n",
            "NBA: \n",
            " - Golden State Warriors\n",
            " - Los Angeles Lakers\n",
            " - Philadelphia 76ers\n",
            "\n",
            "NFL: \n",
            " - Kansas City Chiefs\n",
            " - Green Bay Packers\n",
            " - New England Patriots\n",
            "\n",
            "NHL: \n",
            " - Toronto Maple Leafs\n",
            " - Montreal Canadiens\n",
            " - Colorado Avalanche\n",
            "\n",
            "Or are you interested in a different sport or team? Let me know the details, and I'll be happy to help you with the player listings. \n"
          ]
        }
      ],
      "source": [
        "llm= Cohere(temperature=0, max_tokens=512)\n",
        "\n",
        "print(llm.invoke(\"When did India win the t20 cricket World Cup? \"))\n",
        "print(llm.invoke(\"Give me some players in that team\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypy4zF3R8D00"
      },
      "source": [
        "## ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gzOa5W4171qX"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fSRIYudo8f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8fcf1d-29cb-42cb-dc9e-25dbbf1415b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-024068ee8033>:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-8-024068ee8033>:14: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The 2022 T20 Cricket World Cup was jointly won by India and Pakistan, who competed in the final match on November 13, 2022  As a result, both teams lifted the trophy and were declared as joint winners. \n",
            "\n",
            "This was the first time India has won the T20 Cricket World Cup since its inception in 2007 and the first time ever that two teams have shared the trophy. \n",
            "\n",
            "Would you like to know more about this tournament? I can provide some highlights from the final match if you'd like! \n",
            " Definitely. The Indian team that won the 2022 T20 Cricket World Cup consisted of the following players: \n",
            "\n",
            "- Rohit Sharma (Captain)\n",
            "- Virat Kohli\n",
            "- KL Rahul\n",
            "- Suryakumar Yadav\n",
            "- Rishabh Pant\n",
            "- Axar Patel\n",
            "- Dinesh Karthik\n",
            "- Bhuvneshwar Kumar\n",
            "- Ravichandran Ashwin\n",
            "- Ravindra Jadeja\n",
            "- Jasprit Bumrah\n",
            "- Mohammed Shami\n",
            "- Hardik Pandya\n",
            "- KL Rahul (wicket keeper)\n",
            "\n",
            "This was the first T20 World Cup win for most of these players and a huge milestone for Indian cricket. \n",
            "\n",
            "I hope that was helpful! Do you have a favorite cricketer amongst them? If you'd like to know more about them or any other players, just let me know. \n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import Cohere\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "import os\n",
        "\n",
        "\n",
        "# Initialize Cohere LLM\n",
        "llm = Cohere(model=\"command\", temperature=0.3, max_tokens=512)\n",
        "\n",
        "# Set up memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Set up conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "# Ask questions\n",
        "print(conversation.predict(input=\"When did India win the T20 Cricket World Cup?\"))\n",
        "print(conversation.predict(input=\"Give me some players in that team\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHLvo5ZPC6V"
      },
      "source": [
        "## With Verbose=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kEDfyccEPFsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c325f4-c5e0-4035-a133-06b3ebec7a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: When did India win the T20 Cricket World Cup?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " The 2022 T20 Cricket World Cup was jointly won by India and Pakistan on November 13, 2022. This was India's first T20 World Cup win since the tournament's inception in 2007, and India had previously won the 2011 ODI Cricket World Cup. \n",
            "\n",
            "Would you like to know more about cricket? I can provide information about the rules of the game or other World Cup tournaments if you like! \n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: When did India win the T20 Cricket World Cup?\n",
            "AI:  The 2022 T20 Cricket World Cup was jointly won by India and Pakistan on November 13, 2022. This was India's first T20 World Cup win since the tournament's inception in 2007, and India had previously won the 2011 ODI Cricket World Cup. \n",
            "\n",
            "Would you like to know more about cricket? I can provide information about the rules of the game or other World Cup tournaments if you like! \n",
            "Human: Give me some players in that team\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Absolutely! India's T20 World Cup winning team included the following players: \n",
            "\n",
            "For the men's squad: \n",
            "\n",
            "- Rohit Sharma (captain)\n",
            "- Virat Kohli\n",
            "- Jasprit Bumrah \n",
            "- Ravindra Jadeja\n",
            "- Rishabh Pant\n",
            "- Hardik Pandya\n",
            "- KL Rahul\n",
            "- Mohammed Shami\n",
            "- Axar Patel\n",
            "- Suryakumar Yadav\n",
            "- Prasidh Krishna\n",
            "- R Ashwin\n",
            "\n",
            "And for the women's team: \n",
            "\n",
            "- Harmanpreet Kaur (captain)\n",
            "- Smriti Mandhana\n",
            "- Poonam Yadav\n",
            "- Jemimah Rodrigues\n",
            "- Deepti Sharma\n",
            "- Richa Ghosh\n",
            "- Shikha Pandey\n",
            "- Arundhati Reddy\n",
            "- Komal Zanzad\n",
            "\n",
            "Both teams played exceptionally well, and these are just some of the key players that contributed to their respective victories. \n",
            "\n",
            "Let me know if you'd like more information on any of these players or if you'd like to know more about cricket in general! \n"
          ]
        }
      ],
      "source": [
        "# Initialize Cohere LLM\n",
        "llm = Cohere(model=\"command\", temperature=0.3, max_tokens=512)\n",
        "\n",
        "# Set up memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Set up conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Ask questions\n",
        "print(conversation.predict(input=\"When did India win the T20 Cricket World Cup?\"))\n",
        "print(conversation.predict(input=\"Give me some players in that team\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o810g19hW9HU"
      },
      "source": [
        "### What happens behind the scenes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WZIZ074aW0sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5eb1253-7f74-4dd2-8867-ae04d1de56ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: When did India win the T20 Cricket World Cup?\n",
            "AI:  The 2022 T20 Cricket World Cup was jointly won by India and Pakistan on November 13, 2022. This was India's first T20 World Cup win since the tournament's inception in 2007, and India had previously won the 2011 ODI Cricket World Cup. \n",
            "\n",
            "Would you like to know more about cricket? I can provide information about the rules of the game or other World Cup tournaments if you like! \n",
            "Human: Give me some players in that team\n",
            "AI:  Absolutely! India's T20 World Cup winning team included the following players: \n",
            "\n",
            "For the men's squad: \n",
            "\n",
            "- Rohit Sharma (captain)\n",
            "- Virat Kohli\n",
            "- Jasprit Bumrah \n",
            "- Ravindra Jadeja\n",
            "- Rishabh Pant\n",
            "- Hardik Pandya\n",
            "- KL Rahul\n",
            "- Mohammed Shami\n",
            "- Axar Patel\n",
            "- Suryakumar Yadav\n",
            "- Prasidh Krishna\n",
            "- R Ashwin\n",
            "\n",
            "And for the women's team: \n",
            "\n",
            "- Harmanpreet Kaur (captain)\n",
            "- Smriti Mandhana\n",
            "- Poonam Yadav\n",
            "- Jemimah Rodrigues\n",
            "- Deepti Sharma\n",
            "- Richa Ghosh\n",
            "- Shikha Pandey\n",
            "- Arundhati Reddy\n",
            "- Komal Zanzad\n",
            "\n",
            "Both teams played exceptionally well, and these are just some of the key players that contributed to their respective victories. \n",
            "\n",
            "Let me know if you'd like more information on any of these players or if you'd like to know more about cricket in general! \n"
          ]
        }
      ],
      "source": [
        "print(conversation.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qlz4E-CPGTd"
      },
      "source": [
        "## Memory Example-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VhTgmXyuOmBh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9a52302a-22d8-41fe-f940-0efd2a15e9b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Classification and regression are two of the most fundamental types of machine learning algorithms, but they are used in different ways for different types of problems. In very simple terms, classification is used to predict discrete values, like yes/no or true/false outputs, while regression is used to predict continuous values, like predicting how much something will sell for, or how many people will attend an event. Here are some other key differences:\\n\\n1. Goal: The primary difference is in the objective of these techniques. Classification models aim to predict the membership of a given sample in one or more specific classes or categories. Regression models, on the other hand, aim to predict the relationship between a dependent variable and one or more independent variables. \\n\\n2. Output: Classification algorithms output the predicted probability of belonging to a certain class or category. The output of regression algorithms, on the other hand, is typically a single value, like an amount or a score. \\n\\n3. Error function: Error is measured differently in each technique. In classification, the error is usually measured by the difference between the predicted classification and the true classification, often using a metric like accuracy, precision, recall or F1 score. In regression, the error is typically measured by the difference between the predicted value and the true value, and this is often done using the difference between these values, often calculated using Euclidean distance, or by calculating the mean squared error. \\n\\nThese are some of the main differences between these two types of machine learning models, but they are broad categories, and each can further be broken down into many different specific types of models, each with their own unique properties. \\n\\nI hope this is helpful, is there anything else you'd like to know about these techniques, or about machine learning? \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "llm = Cohere(model=\"command\", temperature=0.3, max_tokens=512)\n",
        "\n",
        "# Set up memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Set up conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "\n",
        "conversation.predict(input=\"Explain three main differences Classification and Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zEdXZrc49XdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8b52a71c-80dc-45bd-94a9-d649cb087998"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" # Classification and Regression: Comparing Key Differences\\n| Difference | Classification | Regression |\\n|---|---|---|\\n| Goal | Predict sample membership in one or more specific classes or categories | Predict the relationship between a dependent variable and one or more independent variables |\\n| Output | Predicted probability of belonging to a certain class or category | Typically, a single value, like an amount or a score |\\n| Error Function | Difference between predicted and true classification (metrics like accuracy, precision, recall or F1 score) | Difference between predicted and true value (Euclidean distance, mean squared error) |\\n\\nClassification and regression are two core types of machine learning algorithms, each with distinct purposes and outcomes. Feel free to adapt this markdown table to your needs. If you'd like me to explain other differences or provide further context, let me know! \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "conversation.predict(input=\"Give the above output in markdown tabular code format \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjMfwfGzAc7o"
      },
      "source": [
        "  | Main Differences | Classification | Regression |\n",
        "| --- | --- | --- |\n",
        "| Type of Learning | Supervised | Supervised |\n",
        "| Predicted Values | Categorical or discrete | Continuous numerical |\n",
        "| Techniques | Decision trees, support vector machines, neural networks | Linear regression, logistic regression, polynomial regression |\n",
        "| Evaluation Metrics | Accuracy, precision, recall | Mean squared error, mean absolute error, R-squared |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uUcAWphy9mGx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "0696caaa-7128-4372-f0e7-df8cbe0d7dd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" In customer success and retention scenarios, classification algorithms are most frequently applied to predict customer churn.\\n\\nClassification models are generally more suitable for binary scenarios, such as predicting whether a customer will churn or not. There are several factors that contribute to this prevalence:\\n\\n1. Simplicity: Classification models are easier for people to understand, and customer churn is often a yes/no scenario.\\n\\n2. Data Availability: Often, businesses have ample data on their customers' behaviors, sentiments, and demographics, but these data points may be discrete or categorical in nature, lending themselves better to classification analysis. \\n\\n3. Business Objective: Customer churn is a critical business challenge for many companies, and a major focus of business strategy. The consequential nature of this problem often drives a need for a more definitive yes/no assessment. \\n\\nHowever, it's crucial to note that the specific model employed will depend on the context of the problem and the data available.\\n\\nIn the case of customer churn, it's worth noting that a regression-based model might not provide the best fit, since it would need to interpret a continuous value in a way that translates to customer churn expectations, which may not be straightforward. \\n\\nThere are various effective techniques used to predict customer churn, such as clustering algorithms to group similar customers based on features like demographics or purchase behavior, or natural language processing (NLP) techniques to understand customer sentiments from communication data. These techniques often feed into a classification problem. \\n\\nThe specific model chosen will depend on the nature of the data, the business context, and the objectives of the analysis. \\n\\nIf you'd like, we can delve into more specific models used in these scenarios, or discuss further considerations for these types of predictive models. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "conversation.predict(input=\"Which one is most widely used in predicting customer churn?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sCKFcBDI90xL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b1b8cddd-c8a8-4bd1-d87c-0692dfe53d0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' One more key difference between classification and regression models is:\\n\\nModel Interpretability:\\n\\nClassification models are generally more interpretable and easier to explain to stakeholders. The classifications assign categories or labels to data points, and these labels can be readily understood by business users. For example, a classification model might classify customers into \"High-Risk\" or \"Low-Risk\" categories, which is straightforward and intuitive.\\n\\nIn contrast, regression models are often more complex, and their results may not be as easily interpreted. A regression line or curve represents a relationship between variables, but this relationship can be more difficult to understand and interpret, especially if there are many variables involved. While regression models may provide more precise predictions, their interpretability is more limited when compared to classification models. \\n\\nThis contrast in interpretability has implications for the realm of user-friendliness, transparency, and communication for different stakeholders involved in interpreting the results or making business decisions based on these models. \\n\\nAgain, the choice between a classification or regression model depends on the specific problem, data characteristics, and preferences of the audience involved in the model\\'s deployment. \\nLet me know if you\\'d like to discuss this further or elaborate on any of the other differences I mentioned above. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "conversation.predict(input=\"Give me one more difference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "s6hvFhF6BRK2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d393590d-b142-467d-ff49-d870d54d0da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Explain three main differences Classification and Regression\n",
            "AI:  Classification and regression are two of the most fundamental types of machine learning algorithms, but they are used in different ways for different types of problems. In very simple terms, classification is used to predict discrete values, like yes/no or true/false outputs, while regression is used to predict continuous values, like predicting how much something will sell for, or how many people will attend an event. Here are some other key differences:\n",
            "\n",
            "1. Goal: The primary difference is in the objective of these techniques. Classification models aim to predict the membership of a given sample in one or more specific classes or categories. Regression models, on the other hand, aim to predict the relationship between a dependent variable and one or more independent variables. \n",
            "\n",
            "2. Output: Classification algorithms output the predicted probability of belonging to a certain class or category. The output of regression algorithms, on the other hand, is typically a single value, like an amount or a score. \n",
            "\n",
            "3. Error function: Error is measured differently in each technique. In classification, the error is usually measured by the difference between the predicted classification and the true classification, often using a metric like accuracy, precision, recall or F1 score. In regression, the error is typically measured by the difference between the predicted value and the true value, and this is often done using the difference between these values, often calculated using Euclidean distance, or by calculating the mean squared error. \n",
            "\n",
            "These are some of the main differences between these two types of machine learning models, but they are broad categories, and each can further be broken down into many different specific types of models, each with their own unique properties. \n",
            "\n",
            "I hope this is helpful, is there anything else you'd like to know about these techniques, or about machine learning? \n",
            "Human: Give the above output in markdown tabular code format \n",
            "AI:  # Classification and Regression: Comparing Key Differences\n",
            "| Difference | Classification | Regression |\n",
            "|---|---|---|\n",
            "| Goal | Predict sample membership in one or more specific classes or categories | Predict the relationship between a dependent variable and one or more independent variables |\n",
            "| Output | Predicted probability of belonging to a certain class or category | Typically, a single value, like an amount or a score |\n",
            "| Error Function | Difference between predicted and true classification (metrics like accuracy, precision, recall or F1 score) | Difference between predicted and true value (Euclidean distance, mean squared error) |\n",
            "\n",
            "Classification and regression are two core types of machine learning algorithms, each with distinct purposes and outcomes. Feel free to adapt this markdown table to your needs. If you'd like me to explain other differences or provide further context, let me know! \n",
            "Human: Which one is most widely used in predicting customer churn?\n",
            "AI:  In customer success and retention scenarios, classification algorithms are most frequently applied to predict customer churn.\n",
            "\n",
            "Classification models are generally more suitable for binary scenarios, such as predicting whether a customer will churn or not. There are several factors that contribute to this prevalence:\n",
            "\n",
            "1. Simplicity: Classification models are easier for people to understand, and customer churn is often a yes/no scenario.\n",
            "\n",
            "2. Data Availability: Often, businesses have ample data on their customers' behaviors, sentiments, and demographics, but these data points may be discrete or categorical in nature, lending themselves better to classification analysis. \n",
            "\n",
            "3. Business Objective: Customer churn is a critical business challenge for many companies, and a major focus of business strategy. The consequential nature of this problem often drives a need for a more definitive yes/no assessment. \n",
            "\n",
            "However, it's crucial to note that the specific model employed will depend on the context of the problem and the data available.\n",
            "\n",
            "In the case of customer churn, it's worth noting that a regression-based model might not provide the best fit, since it would need to interpret a continuous value in a way that translates to customer churn expectations, which may not be straightforward. \n",
            "\n",
            "There are various effective techniques used to predict customer churn, such as clustering algorithms to group similar customers based on features like demographics or purchase behavior, or natural language processing (NLP) techniques to understand customer sentiments from communication data. These techniques often feed into a classification problem. \n",
            "\n",
            "The specific model chosen will depend on the nature of the data, the business context, and the objectives of the analysis. \n",
            "\n",
            "If you'd like, we can delve into more specific models used in these scenarios, or discuss further considerations for these types of predictive models. \n",
            "Human: Give me one more difference\n",
            "AI:  One more key difference between classification and regression models is:\n",
            "\n",
            "Model Interpretability:\n",
            "\n",
            "Classification models are generally more interpretable and easier to explain to stakeholders. The classifications assign categories or labels to data points, and these labels can be readily understood by business users. For example, a classification model might classify customers into \"High-Risk\" or \"Low-Risk\" categories, which is straightforward and intuitive.\n",
            "\n",
            "In contrast, regression models are often more complex, and their results may not be as easily interpreted. A regression line or curve represents a relationship between variables, but this relationship can be more difficult to understand and interpret, especially if there are many variables involved. While regression models may provide more precise predictions, their interpretability is more limited when compared to classification models. \n",
            "\n",
            "This contrast in interpretability has implications for the realm of user-friendliness, transparency, and communication for different stakeholders involved in interpreting the results or making business decisions based on these models. \n",
            "\n",
            "Again, the choice between a classification or regression model depends on the specific problem, data characteristics, and preferences of the audience involved in the model's deployment. \n",
            "Let me know if you'd like to discuss this further or elaborate on any of the other differences I mentioned above. \n"
          ]
        }
      ],
      "source": [
        "#What happens behind the scenes?\n",
        "print(conversation.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSPeHmsUoJ4"
      },
      "source": [
        "## ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZiPXoA0YCT6x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8cb38ad1-7b5b-4bc7-90cf-9ef8d9f33d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-a4a7100266f7>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello! I'd be happy to assist you with your order. Could you please provide me with your order number so I can look up the specific details? It would also help if you could let me know the details of the assistance you need with your order. \\n\\nIf you need help with something that requires real-time assistance, such as a cancellation or a modification to an order, it would be helpful to know whether you want to change or cancel your order. \\n\\nI will do my best to assist you with your specific needs. Please let me know how I can help you further. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "conversation.predict(input=\"Hello, I need help with my order\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xbkNDQChUADo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d66fe97d-78d7-4cf6-d43a-01bf03b5d5e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Thank you for your order! We are ready to assist you with any needs you may have. To better assist you, may I please have your order number? If you need assistance with the order, such as real-time assistance with a cancellation or modification, we are equipped to do so. We want to make sure you get exactly what you need. \\n\\nPlease let me know if you have any specific needs or questions about your order. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "conversation.predict(input=\"I've ordered a pizza\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6t6jolCMUIyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "546468c4-d29f-44c4-d323-faff5e2df133"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" That's okay! The order number would've been in your confirmation email or text, as long as you used a valid contact method to place the order. All our orders are tracked with a unique order number throughout their journey, from the moment you place the order, through fulfillment and delivery. If you can't find the confirmation email, it may have ended up in your spam folder, so be sure to check there too. If you're having trouble locating your order number, no worries, I can still help you with any modifications or cancellations, however, having the order number would expedite the process. \\n\\nIf you did not use a registered account to place the order, then there is no order history kept with us, and you would need to reach out to the restaurant directly to make any changes. If you placed the order with a valid account, you can also use the support section on our website to reach support through a live chat or phone. \\n\\nIf you would like me to forward this message to the restaurant directly, could you please confirm the restaurant's name, the time, and the approximate total of the order? I can imagine that the sooner you cancel the order, the better, so that the restaurant can optimize their prep and delivery resources. I will do my best to assist you in this situation if you are unsure of how to move forward. \\nLet me know if you have any other questions about your order or our services! \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "conversation.predict(input=\"I don't remember the order details\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MsK75HS8UgDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010bb64c-dffe-4d0c-fd3b-555b2a926c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The human tells the AI they have ordered a pizza and asks for the order number. The AI requests the order number and offers assistance with any modifications or cancellations, promising to help the human with their specific needs. The AI then provides options to locate the order number, offers to directly message the restaurant for order modifications or cancellations, and reassures the human about the process to optimize any concerns regarding the order. \n"
          ]
        }
      ],
      "source": [
        "print(conversation.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7UbWDAsP-gI"
      },
      "source": [
        "## ConverstationEntitySummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IgEGRiTfRUHk"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
        "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DCk4YwWPR5SE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2548a4b2-c315-4dbd-ec65-4aa18a808ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{entities}\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Last line:\n",
            "Human: {input}\n",
            "You:\n"
          ]
        }
      ],
      "source": [
        "template=ENTITY_MEMORY_CONVERSATION_TEMPLATE.template\n",
        "print(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_x7qSkN7QBqa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "798c9bba-266b-4478-d67a-3da8efa02b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-3848c8f9d764>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationEntityMemory(llm=llm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Venkat': '', 'Bangalore': ''}\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Last line:\n",
            "Human: Hello, My Name is Venkat. I am from Bangalore. I need help with my order\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Venkat, glad to meet you and happy to help with your query. Please let me know what order you are referring to so I can provide you with the required assistance. I'll do my best to ensure you get the assistance you require. \\n\\nIf you could provide more details regarding your query, it would help me provide a more accurate response tailored to your specific needs. Feel free to ask if you have any other questions or concerns, and I'll provide the necessary assistance to the best of my capabilities. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "memory = ConversationEntityMemory(llm=llm)\n",
        "conversation = ConversationChain(llm=llm,\n",
        "                                 memory=memory,\n",
        "                                 verbose=True,\n",
        "                                 prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE)\n",
        "conversation.predict(input=\"Hello, My Name is Venkat. I am from Bangalore. I need help with my order\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "h4aPc45rRB0F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "3e793012-daca-43d7-92d7-6d0fbaf9686a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Mobile': '', 'tuesday': '', 'night': '', '8pm': '', 'Venkat': 'Venkat is a customer who is reaching out for assistance with an order they placed.', 'Bangalore': \"Bangalore is Venkat's place of origin and current residence, and is known for its vibrant culture, rich history, and IT sector. It is a hub for technological innovation and advancement.\"}\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My Name is Venkat. I am from Bangalore. I need help with my order\n",
            "AI:  Hello Venkat, glad to meet you and happy to help with your query. Please let me know what order you are referring to so I can provide you with the required assistance. I'll do my best to ensure you get the assistance you require. \n",
            "\n",
            "If you could provide more details regarding your query, it would help me provide a more accurate response tailored to your specific needs. Feel free to ask if you have any other questions or concerns, and I'll provide the necessary assistance to the best of my capabilities. \n",
            "Last line:\n",
            "Human: I ordered Mobile phone on tuesday night 8pm\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Thank you for providing this information. We can certainly assist you in checking the status of your mobile phone order placed on Tuesday night at 8pm. Could you please provide us with your order number so that we can promptly initiate the necessary checks? \\n\\nIf there is anything else I can assist you with, please let me know. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "conversation.predict(input=\"I ordered Mobile phone on tuesday night 8pm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jqgoqGd_S0lG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "c745c0d6-c043-4f8f-abdc-f1ea0ac89702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'IBLC19678456': '', 'Venkat': 'Venkat is a customer who is reaching out for assistance with an order they placed for a mobile phone on Tuesday, 8pm. They are from Bangalore.', 'Bangalore': 'Bangalore is the place of residence of the user Venkat, who ordered a mobile phone on Tuesday night at 8 pm.'}\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My Name is Venkat. I am from Bangalore. I need help with my order\n",
            "AI:  Hello Venkat, glad to meet you and happy to help with your query. Please let me know what order you are referring to so I can provide you with the required assistance. I'll do my best to ensure you get the assistance you require. \n",
            "\n",
            "If you could provide more details regarding your query, it would help me provide a more accurate response tailored to your specific needs. Feel free to ask if you have any other questions or concerns, and I'll provide the necessary assistance to the best of my capabilities. \n",
            "Human: I ordered Mobile phone on tuesday night 8pm\n",
            "AI:  Thank you for providing this information. We can certainly assist you in checking the status of your mobile phone order placed on Tuesday night at 8pm. Could you please provide us with your order number so that we can promptly initiate the necessary checks? \n",
            "\n",
            "If there is anything else I can assist you with, please let me know. \n",
            "Last line:\n",
            "Human: My order number is IBLC19678456. What is the status of my order?\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Thank you for providing the order number, IBLC19678456, which enables me to review the details associated with your order and provide you with the up-to-date status.\\n\\nBased on our records, your order has been successfully processed and the item(s) are currently scheduled for delivery today. You can expect the package to arrive at your designated address by the end of the day. \\n\\nIf for any reason you have not received the package or face any other issues with the delivery, please contact us immediately so we can address the matter and ensure a smooth delivery process. \\n\\nWe appreciate your patience and understanding while we work to get your order to you as soon as possible. \\n\\nIs there anything else I can assist you with? Please let me know if you have any further questions or concerns about your order and I'll be here to help. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "conversation.predict(input=\"My order number is IBLC19678456. What is the status of my order?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YXYMQGG7S2mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a2b8f4-6a7e-4091-a8f9-e0e7e54e4e55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hello, My Name is Venkat. I am from Bangalore. I need help with my order', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\" Hello Venkat, glad to meet you and happy to help with your query. Please let me know what order you are referring to so I can provide you with the required assistance. I'll do my best to ensure you get the assistance you require. \\n\\nIf you could provide more details regarding your query, it would help me provide a more accurate response tailored to your specific needs. Feel free to ask if you have any other questions or concerns, and I'll provide the necessary assistance to the best of my capabilities. \", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I ordered Mobile phone on tuesday night 8pm', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=' Thank you for providing this information. We can certainly assist you in checking the status of your mobile phone order placed on Tuesday night at 8pm. Could you please provide us with your order number so that we can promptly initiate the necessary checks? \\n\\nIf there is anything else I can assist you with, please let me know. ', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='My order number is IBLC19678456. What is the status of my order?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\" Thank you for providing the order number, IBLC19678456, which enables me to review the details associated with your order and provide you with the up-to-date status.\\n\\nBased on our records, your order has been successfully processed and the item(s) are currently scheduled for delivery today. You can expect the package to arrive at your designated address by the end of the day. \\n\\nIf for any reason you have not received the package or face any other issues with the delivery, please contact us immediately so we can address the matter and ensure a smooth delivery process. \\n\\nWe appreciate your patience and understanding while we work to get your order to you as soon as possible. \\n\\nIs there anything else I can assist you with? Please let me know if you have any further questions or concerns about your order and I'll be here to help. \", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "conversation.memory.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BLGNPWJaTB5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38865ef0-b3d5-433a-debb-07f9afc8c1c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InMemoryEntityStore(store={'Venkat': 'Venkat is a customer who is reaching out for assistance with an order they placed for a mobile phone on Tuesday, 8pm. They are from Bangalore, and their order number is IBLC19678456. The order has been successfully processed and the mobile phone is scheduled for delivery today.', 'Bangalore': 'Bangalore is the place of residence of the user Venkat, who ordered a mobile phone on Tuesday night at 8 pm and whose order number is IBLC19678456. The order has been successfully processed and the mobile phone is scheduled for delivery today.', 'Mobile': 'Venkat ordered a mobile phone on Tuesday, 8pm. They need assistance in checking the status of the order.', 'tuesday': 'Tuesday night at 8 pm is when Venkat ordered a mobile phone.', 'night': 'Tuesday night is significant because that is the time when the mobile phone order was placed by the user.', '8pm': '8pm is the time that Venkat ordered a mobile phone on Tuesday night.', 'IBLC19678456': 'Order number IBLC19678456 was placed on Tuesday night at 8pm and successfully processed. The mobile phone order is scheduled for delivery today.'})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "conversation.memory.entity_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtOyURrOcyfN"
      },
      "source": [
        "# Building a Simple Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "I3vLanZ2Sov6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import Cohere\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wk6BiZ-lc9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2feb2c-ddab-43c1-e9f9-0c79a8890279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your message: hey,whats the weather in coimbatore today\n",
            "AI message ==>  The weather in Coimbatore today is mostly cloudy with a high of 71 degrees Fahrenheit and a low of 67 degrees Fahrenheit. There is a 15% chance of precipitation today but it is not expected to rain heavily. \n",
            "Would you like to know about the weather this coming week so you can plan your outfits? I can also provide you with suggestions based on the weather if you would like.\n",
            "\n",
            "Your message: I want to learn genAI where should i start with\n",
            "AI message ==>  There are many resources available to help you learn about Genetic AI, also known as Evolutionary AI. Since Genetic Al goes by another name, here are some specific avenues you can take to learn about this topic:\n",
            "\n",
            "1. Start with online research: You can begin by reading Wikipedia articles on Genetic Algorithms (GA) and Evolutionary Algorithms (EA). These provide an overview of the concepts, and the wikipedia page also lists relevant books and papers if you would like further reading on the topic. \n",
            "\n",
            "2. Take a course: There are many courses available online that cover genetic algorithms and evolutionary intelligence. You could take a look at online learning platforms like Coursera, Udemy, or edX, and search for courses on \"genetic algorithms\" or \"evolutionary algorithms.\" You could also look at the course material and syllabi for relevant courses at universities like Stanford, MIT, or Carnegie Mellon, and use the titles of relevant courses to guide your learning and find appropriate resources or textbooks. \n",
            "\n",
            "3. Read relevant books: There are many books that cover genetic algorithms and evolutionary intelligence in depth. For example, \"Genetic Algorithms in Search, Optimization, and Machine Learning\" by John Holland, or \"Evolutionary Computation: Principles and Practice\" by John Koza and Rick Westerman. \n",
            "\n",
            "4. Learn about its Applications: Genetic AI has many applications in diverse fields like optimization problems, data analysis, and robotics. You can select a specific application of interest to you and study its application in that domain. For example, you could look at how genetic algorithms are used to create generative art by selecting a few initial examples and letting the algorithm evolve new pieces based on an objective score, or how they can be used to write stories, which can be quite amazing and interesting. \n",
            "\n",
            "5. Practice implementing basic algorithms: Implementing your own genetic algorithm can help you better understand the underlying concepts, even if the task is quite daunting and the initial results may seem lackluster. There are many online resources that can help you get started, and you can start with something simple, such as an algorithm that optimizes a function of one variable, or something more complex but more interesting, such as evolving images or robot behaviors. \n",
            "\n",
            "Remember that Genetic AI is a complex and diverse topic, and it may take some time and effort to understand the basic and advanced concepts, as well as to understand which aspects of the field are most relevant to the specific type of AI you are interested in. Let me know if you would\n",
            "Your message: quit\n"
          ]
        }
      ],
      "source": [
        "# Initialize Cohere LLM\n",
        "llm = Cohere(model=\"command\", temperature=0.3, max_tokens=512)\n",
        "\n",
        "# Memory buffer for the conversation\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Initialize conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Start conversation loop\n",
        "user_input = input(\"Your message: \")\n",
        "while user_input.lower() != \"quit\":\n",
        "    response = conversation.predict(input=user_input)\n",
        "    print(\"AI message ==>\", response)\n",
        "    user_input = input(\"Your message: \")\n",
        "\n",
        "# Clear memory on exit\n",
        "memory.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5HjnCHJwFI9"
      },
      "source": [
        "# Example with RAG, Memory and Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iWEpYYzG--Qc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805be3aa-8800-4e09-8f33-3673e73a5d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/303.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/303.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pypdf --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4C7Jewx3wlwd"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFoiLWdwCwgj"
      },
      "source": [
        "## Step-1 :Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dM1yvZvewheE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653a6c2a-a56f-4a04-ccde-13a3ff5133db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-28 14:45:44--  https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/CIBIL_Report/All_Cibil_Docs.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2818524 (2.7M) [application/zip]\n",
            "Saving to: ‘All_Cibil_Docs.zip’\n",
            "\n",
            "All_Cibil_Docs.zip  100%[===================>]   2.69M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-05-28 14:45:44 (35.4 MB/s) - ‘All_Cibil_Docs.zip’ saved [2818524/2818524]\n",
            "\n",
            "Archive:  All_Cibil_Docs.zip\n",
            "  inflating: All_Docs_Set1/5787-15251-1-PB.pdf  \n",
            "  inflating: All_Docs_Set1/CIBIL Score in India.pdf  \n",
            "  inflating: All_Docs_Set1/creditscoretips_2.pdf  \n",
            "  inflating: All_Docs_Set1/MyCIBIL-Understanding.pdf  \n",
            "  inflating: All_Docs_Set1/understanding-cir-ctc.pdf  \n",
            "['All_Docs_Set1/creditscoretips_2.pdf', 'All_Docs_Set1/understanding-cir-ctc.pdf', 'All_Docs_Set1/5787-15251-1-PB.pdf', 'All_Docs_Set1/CIBIL Score in India.pdf', 'All_Docs_Set1/MyCIBIL-Understanding.pdf']\n"
          ]
        }
      ],
      "source": [
        "#Example of ConversationalRetrievalChain\n",
        "\n",
        "#Document Loading\n",
        "!wget https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/CIBIL_Report/All_Cibil_Docs.zip\n",
        "#Unzip and Overwrite\n",
        "!unzip -o All_Cibil_Docs.zip -d All_Docs_Set1\n",
        "#listdown all documents content/All_Docs_Set1 and store them\n",
        "pdf_files = glob.glob(\"All_Docs_Set1/*.pdf\")\n",
        "print(pdf_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tIevEFt7-11C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd796a50-fee5-4cef-e539-8438815d5d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All_Docs_Set1/creditscoretips_2.pdf 1\n",
            "All_Docs_Set1/understanding-cir-ctc.pdf 3\n",
            "All_Docs_Set1/5787-15251-1-PB.pdf 12\n",
            "All_Docs_Set1/CIBIL Score in India.pdf 7\n",
            "All_Docs_Set1/MyCIBIL-Understanding.pdf 4\n",
            "t credit reports to make prescreened credit offers, \n",
            "these inquiries about your credit history are not \n",
            "counted as applications for credit. \n",
            " How many and what types of credit accounts do \n",
            "you have? Many credit-scoring models consider \n",
            "the number and type of credit accounts you have. \n",
            "A mix of installment loans and credit cards may \n",
            "improve your score. However, too many finance \n",
            "company accounts or credit cards might hurt your \n",
            "score.  \n",
            " To learn more about credit scoring, see the Federal \n",
            "Trade Commission’s website, Facts for Consumers \n",
            "at www.ftc.gov/bcp/edu/pubs/consumer/credit/\n",
            "cre24.shtm. \n",
            "4. Learn the legal steps you must take to \n",
            "improve your credit report.\n",
            " The Federal Trade Commission’s “Building a Better \n",
            "Credit Report” (www.ftc.gov/bcp/edu/pubs/\n",
            "consumer/credit/cre03.shtm) has information on cor-\n",
            "recting errors in your report, tips on dealing with debt \n",
            "and avoiding scams—and more.  \n",
            "5. Beware of credit-repair scams.\n",
            " Sometimes doing it yourself is the best way to repair \n",
            "y\n",
            "Lines 898\n",
            "Words 10706\n",
            "Charecters 67460\n"
          ]
        }
      ],
      "source": [
        "full_text = \"\"\n",
        "for pdf_file in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_file)\n",
        "    pages = loader.load()\n",
        "    print(pdf_file, len(pages))\n",
        "    for page in pages:\n",
        "        full_text += page.page_content\n",
        "\n",
        "print(full_text[1:1000])\n",
        "print(\"Lines\" , len(full_text.split(\"\\n\")))\n",
        "print(\"Words\" , len(full_text.split(\" \")))\n",
        "print(\"Charecters\", len(full_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nddR_wGDElT"
      },
      "source": [
        "## Step-2:Split the data into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "t7vJzrSgC1Lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7188abd-9533-4413-9294-24f4ee76f967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291\n",
            "at credit reports to make prescreened credit offers, \n",
            "these inquiries about your credit history are not \n",
            "counted as applications for credit. \n",
            " How many and what types of credit accounts do \n",
            "you have? Many credit-scoring models consider \n",
            "the number and type of credit accounts you have.\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=60)\n",
        "chunks = text_splitter.split_text(full_text)\n",
        "print(len(chunks))\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciwB4WJlFrr8"
      },
      "source": [
        "## Step-3: Creating embeddings and Storing in Vector Stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YPb15teWG1Bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0350bb-28b3-4054-f1ed-f85acec4e7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install ChromaDB --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-cohere cohere\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htd6LELJXbdq",
        "outputId": "b1962e25-717d-4b0d-af7d-5795903793c3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-cohere\n",
            "  Downloading langchain_cohere-0.4.4-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.11/dist-packages (5.15.0)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (0.3.24)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (0.3.60)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (2.11.4)\n",
            "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from langchain-cohere)\n",
            "  Downloading types_pyyaml-6.0.12.20250516-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere) (1.11.1)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.0.20250515)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.42)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.31.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.8)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.0)\n",
            "Downloading langchain_cohere-0.4.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_pyyaml-6.0.12.20250516-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-pyyaml, langchain-cohere\n",
            "Successfully installed langchain-cohere-0.4.4 types-pyyaml-6.0.12.20250516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ssiV5nucAQol"
      },
      "outputs": [],
      "source": [
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import os\n",
        "\n",
        "# Set your Cohere API Key\n",
        "# os.environ[\"COHERE_API_KEY\"] = \"your-cohere-api-key\"\n",
        "\n",
        "# Initialize Cohere Embeddings\n",
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
        "\n",
        "# Create Chroma Vector Store\n",
        "cibil_db = Chroma.from_texts(\n",
        "    texts=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"cibil_db\"\n",
        ")\n",
        "\n",
        "# Persist the database\n",
        "cibil_db.persist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "p_egFbt8UN4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821d0f1b-5b0b-43cf-926f-2d9b1f8b70f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score, which is widely used by lenders to \n",
            "evaluate your loan applications. CIBIL \n",
            "Score ranges from 300-900, and is \n",
            "derived by using details found in the \n",
            "‘Accounts’ and ‘Enquiries’ sections of \n",
            "your CIBIL Report. The closer your \n",
            "score is to 900, the more confidence\n",
            "plays a crucial role in the lending process, influencing lenders' decisions on whether \n",
            "to extend credit to a prospective borrower. \n",
            "2. What is CIBIL Score? \n",
            "A CIBIL Score is a three-digit number, ranging from 300 to 900, that reflects a\n",
            "which is widely used by loan providers to \n",
            "evaluate loan applications. An individual’s \n",
            "CIBIL Score ranges between 300-900 \n",
            "and is calculated basis the information in \n",
            "the “Accounts” and “Enquiry” section of \n",
            "the credit report. The closer the score to \n",
            "900, the more confidence the loan\n",
            "Credit EducationWhat is a CIBIL Score?\n",
            "CIBIL Score is a 3 digit numeric summary of your credit history, derived by using details found in the ‘Accounts’ and ‘Enquiries’ \n",
            "sections on your CIBIL Report and ranges from 300 to 900. The closer your score is to 900, the higher are the chances of your\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-53cf5febf39f>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result=retriever.get_relevant_documents(query=\"What is the CIBIL Score?\")\n"
          ]
        }
      ],
      "source": [
        "retriever = cibil_db.as_retriever()\n",
        "result=retriever.get_relevant_documents(query=\"What is the CIBIL Score?\")\n",
        "for i in range(len(result)):\n",
        "  print(result[i].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk2tQsfsFyrs"
      },
      "source": [
        "## Step-4: Conversation and Retrieval Chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI, cohere\n",
        "from langchain.llms import Cohere"
      ],
      "metadata": {
        "id": "tqMgT-PxJLc2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kLrb6fpwFpk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92f4ed3-db49-4d91-8d42-fa49b4fe97da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-6ac06aeb4561>:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = conversational_RAG.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: The CIBIL score mentioned in the report is a **three-digit numeric summary** ranging from **300 to 900**. It is derived using details found in the **‘Accounts’** and **‘Enquiries’** sections of the CIBIL Report. The closer the score is to 900, the higher the confidence or chances of loan approval.\n"
          ]
        }
      ],
      "source": [
        "from langchain_cohere import ChatCohere  # ✅ Import the correct Cohere chat model\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# Initialize Cohere chat model\n",
        "llm = ChatCohere(temperature=0.3, max_tokens=512)  # You can tweak temperature as needed\n",
        "\n",
        "# Initialize memory for the conversation\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# Build the Conversational RAG chain\n",
        "conversational_RAG = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=cibil_db.as_retriever(),  # Assumes Chroma vector DB is already initialized\n",
        "    memory=memory,\n",
        "    # verbose=True  # Optional: Enable for debugging\n",
        ")\n",
        "\n",
        "# Test the chain\n",
        "query = \"What is the CIBIL score mentioned in the report?\"\n",
        "response = conversational_RAG.run(query)\n",
        "print(\"Response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr8Bc1ySIIQU"
      },
      "source": [
        "## Step-5 : Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qqOdnx80FpoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61e777a-736c-4a1a-fb47-d87349257f23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your message:hey,give about genai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-c445e2efa08c>:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response=conversational_RAG({\"question\": user_input})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI message ==> Lo siento, no tengo información sobre \"GenAI\" en el contexto proporcionado. La información que tengo se refiere a datos personales, detalles de contacto, y extractos sobre crédito y finanzas, pero no menciona nada relacionado con \"GenAI\". Si tienes alguna otra pregunta relacionada con los datos proporcionados, estaré encantado de ayudarte.\n",
            "Your message: translate your response to english\n",
            "AI message ==> I don't know the answer to that question based on the provided context. The information given is related to personal details, academic abstracts, and credit scoring, but it does not contain any information about GenAI.\n",
            "Your message: quit\n"
          ]
        }
      ],
      "source": [
        "user_input=input(\"Your message:\")\n",
        "while user_input!=\"quit\":\n",
        "    response=conversational_RAG({\"question\": user_input})\n",
        "    print(\"AI message ==>\", response[\"answer\"])\n",
        "    user_input=input(\"Your message: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG ChatBotTool"
      ],
      "metadata": {
        "id": "DSjFoQYdSCJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "langchain\n",
        "langchain_community\n",
        "PyPDF2\n",
        "python-dotenv\n",
        "streamlit\n",
        "openai\n",
        "faiss-cpu\n",
        "altair\n",
        "tiktoken\n",
        "huggingface-hub\n",
        "InstructorEmbedding\n",
        "sentence-transformers\n",
        "pydantic"
      ],
      "metadata": {
        "id": "B0lEI8A0QjWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c84cc54-ff32-4ec4-8f9d-fb4e4bad2f47"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "id": "pXiNMQMJTezd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b96c29-d4ea-4305-8884-8c28e79b48c6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m796.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip"
      ],
      "metadata": {
        "id": "Xu6oEkcrpM7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# ✅ Use Cohere-specific imports\n",
        "from langchain_cohere import CohereEmbeddings, ChatCohere\n",
        "\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_embeddings(text_chunks):\n",
        "    # ✅ Use CohereEmbeddings instead of OpenAIEmbeddings\n",
        "    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")  # or \"embed-english-light-v3.0\"\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def get_conversational_chain(vectorstore):\n",
        "    # ✅ Use ChatCohere instead of ChatOpenAI\n",
        "    llm = ChatCohere(temperature=0.3, max_tokens=512)\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversational_chain\n",
        "\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "    response = st.session_state.conversational_chain({\"question\": user_question})\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0:\n",
        "            st.write(':man_in_tuxedo:', message.content)\n",
        "        else:\n",
        "            st.write(':robot_face:', message.content)\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Chat with Documents\", page_icon=\":books:\")\n",
        "    st.title(\"AI Chat-Bot for Your Documents :books:\")\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "        <style>\n",
        "            .stTextInput {\n",
        "                position: fixed;\n",
        "                bottom: 3rem;\n",
        "            }\n",
        "        </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    if \"conversational_chain\" not in st.session_state:\n",
        "        st.session_state.conversational_chain = None\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = None\n",
        "\n",
        "    user_input = st.text_input(\"Upload files, Hit Submit button, then ask questions. :blue[- Venkat Reddy] :sunglasses:\")\n",
        "    if user_input:\n",
        "        handle_userinput(user_input)\n",
        "\n",
        "    with st.sidebar:\n",
        "        pdf_docs = st.file_uploader(\"Upload your documents here\", accept_multiple_files=True)\n",
        "        if st.button(\"Submit\"):\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                raw_text = get_pdf_text(pdf_docs)\n",
        "                chunked_array = get_text_chunks(raw_text)\n",
        "                embeddings = get_embeddings(chunked_array)\n",
        "                vectorstore = FAISS.from_texts(chunked_array, embeddings)\n",
        "                st.session_state.conversational_chain = get_conversational_chain(vectorstore)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Lb0BtRnzTkc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dfb9cb-bcca-419f-deff-2e2325995de2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n",
        "#Doesn't work in Chrome sometimes - Due to CSS related issues; Try Forefox"
      ],
      "metadata": {
        "id": "YphGHyMITyNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee19ffd-9031-442d-b136-12596e2284f0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.148.139.250\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.139.250:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20G\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kTWb7gLZIYq",
        "outputId": "317cf0c9-6de7-4bb7-853a-d8733f043a27"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y\n",
            "y\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.9-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok langchain faiss-cpu cohere langchain-cohere PyPDF2\n"
      ],
      "metadata": {
        "id": "t-Ab_9Guq7eq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Paste your token inside the quotes\n",
        "ngrok.set_auth_token(\"2xjQaBduPTaKWhaNGGksD8duqY1_7uh36Nrgbw5gYnDDmGSto\")\n"
      ],
      "metadata": {
        "id": "THdySqdzsC4Y"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio PyPDF2 faiss-cpu langchain langchain-cohere cohere\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LnmpEJ-re-H",
        "outputId": "e7b2b4c2-d452-446f-a17c-4c96eb9c221b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-cohere in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.11/dist-packages (5.15.0)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.9)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (0.3.24)\n",
            "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (6.0.12.20250516)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere) (1.11.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.0.20250515)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.9.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.0)\n",
            "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, safehttpx, gradio-client, gradio\n",
            "Successfully installed aiofiles-24.1.0 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 tomlkit-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the app code\n",
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain_cohere import CohereEmbeddings, ChatCohere\n",
        "\n",
        "# Global variables to hold chain and memory\n",
        "conversational_chain = None\n",
        "chat_history = []\n",
        "\n",
        "# 1. Read PDF text\n",
        "def get_pdf_text(pdf_files):\n",
        "    text = \"\"\n",
        "    for file in pdf_files:\n",
        "        pdf_reader = PdfReader(file.name)\n",
        "        for page in pdf_reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text\n",
        "    return text\n",
        "\n",
        "# 2. Split text into chunks\n",
        "def get_text_chunks(text):\n",
        "    splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "# 3. Create embeddings\n",
        "def get_embeddings():\n",
        "    return CohereEmbeddings(model=\"embed-english-v3.0\")\n",
        "\n",
        "# 4. Create the chain\n",
        "def create_chain(text_chunks):\n",
        "    global conversational_chain, chat_history\n",
        "    chat_history = []\n",
        "    vectorstore = FAISS.from_texts(text_chunks, get_embeddings())\n",
        "    llm = ChatCohere(temperature=0.3, max_tokens=512)\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "\n",
        "# 5. Process uploaded PDFs\n",
        "def process_documents(docs):\n",
        "    raw_text = get_pdf_text(docs)\n",
        "    chunks = get_text_chunks(raw_text)\n",
        "    create_chain(chunks)\n",
        "    return \"✅ Documents processed. You can now ask questions.\"\n",
        "\n",
        "# 6. Chat function\n",
        "def chat_with_bot(user_input):\n",
        "    global conversational_chain, chat_history\n",
        "    if not conversational_chain:\n",
        "        return \"❌ Please upload and process documents first.\"\n",
        "    response = conversational_chain({\"question\": user_input})\n",
        "    chat_history = response['chat_history']\n",
        "    return response['answer']\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 📄 AI Chat-Bot for Your Documents (with Cohere + Gradio)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_upload = gr.File(file_types=[\".pdf\"], file_count=\"multiple\", label=\"Upload your PDFs\")\n",
        "        upload_btn = gr.Button(\"Process Documents\")\n",
        "\n",
        "    output_status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(label=\"Ask a Question\")\n",
        "        send_btn = gr.Button(\"Send\")\n",
        "\n",
        "    answer_output = gr.Textbox(label=\"AI Answer\", lines=6)\n",
        "\n",
        "    upload_btn.click(process_documents, inputs=file_upload, outputs=output_status)\n",
        "    send_btn.click(chat_with_bot, inputs=user_input, outputs=answer_output)\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "z-X78j5_rhaP",
        "outputId": "ff5ec722-ce5a-4629-ac02-d130f1c35a62"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://73566a94518629a942.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://73566a94518629a942.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDeTasCosnD6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}